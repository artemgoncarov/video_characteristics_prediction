{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64\n",
    "import cv2\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import base64\n",
    "import pickle\n",
    "tqdm.pandas()\n",
    "\n",
    "import base64\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(base64_str):\n",
    "    try:\n",
    "        img_data = base64.b64decode(base64_str)\n",
    "        np_arr = np.frombuffer(img_data, np.uint8)\n",
    "        img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            return img\n",
    "    except:\n",
    "        pass\n",
    "    return np.zeros((100, 100, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import graycomatrix, graycoprops\n",
    "import string\n",
    "\n",
    "# def extract_image_features(df):\n",
    "#     img_sizes = []\n",
    "#     img_means = []\n",
    "#     img_stds = []\n",
    "#     # img_min = []\n",
    "#     # img_max = []\n",
    "#     for img_b64 in tqdm(df['photo'].fillna(\"\")):\n",
    "#         img = decode_image(img_b64)\n",
    "#         img_sizes.append(img.shape[0] * img.shape[1])\n",
    "#         img_means.append(img.mean())\n",
    "#         img_stds.append(img.std())\n",
    "#         # img_min.append(img.min())\n",
    "#         # img_max.append(img.max())\n",
    "#     df['img_size'] = img_sizes\n",
    "#     df['img_mean'] = img_means\n",
    "#     df['img_std'] = img_stds\n",
    "#     # df['img_min'] = img_min\n",
    "#     # df['img_max'] = img_max\n",
    "#     df.drop(columns=['photo'], inplace=True)\n",
    "#     return df\n",
    "\n",
    "def extract_image_features(df):\n",
    "    img_sizes = []\n",
    "    img_means = []\n",
    "    img_stds = []\n",
    "    img_mins = []\n",
    "    img_maxs = []\n",
    "    img_entropies = []\n",
    "    \n",
    "    glcm_contrasts = []\n",
    "    glcm_dissimilarities = []\n",
    "    glcm_homogeneities = []\n",
    "    glcm_energies = []\n",
    "    glcm_correlations = []\n",
    "    glcm_ASMs = []\n",
    "    \n",
    "    for img_b64 in tqdm(df['photo'].fillna(\"\")):\n",
    "        img = decode_image(img_b64)\n",
    "        \n",
    "        # If image is color, convert to grayscale for GLCM\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "            # Convert to grayscale if needed (example with np.dot or cv2):\n",
    "            # img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img_gray = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)\n",
    "        else:\n",
    "            # Already grayscale\n",
    "            img_gray = img.astype(np.uint8)\n",
    "        \n",
    "        # 1. Basic pixel intensity features\n",
    "        img_sizes.append(img.shape[0] * img.shape[1])\n",
    "        img_means.append(img.mean())\n",
    "        img_stds.append(img.std())\n",
    "        img_mins.append(img.min())\n",
    "        img_maxs.append(img.max())\n",
    "        \n",
    "        # 2. Entropy (classic definition using histogram)\n",
    "        hist, _ = np.histogram(img.ravel(), bins=256, range=(0, 256))\n",
    "        hist = hist[hist > 0]\n",
    "        probabilities = hist / hist.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        img_entropies.append(entropy)\n",
    "        \n",
    "        # 3. GLCM-based texture features (using angles 0, 45, 90, 135 degrees)\n",
    "        glcm = graycomatrix(\n",
    "            img_gray, \n",
    "            distances=[1], \n",
    "            angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "            levels=256, \n",
    "            symmetric=True, \n",
    "            normed=True\n",
    "        )\n",
    "        \n",
    "        contrast = graycoprops(glcm, 'contrast').mean()\n",
    "        dissimilarity = graycoprops(glcm, 'dissimilarity').mean()\n",
    "        homogeneity = graycoprops(glcm, 'homogeneity').mean()\n",
    "        energy = graycoprops(glcm, 'energy').mean()\n",
    "        correlation = graycoprops(glcm, 'correlation').mean()\n",
    "        ASM = graycoprops(glcm, 'ASM').mean()\n",
    "        \n",
    "        glcm_contrasts.append(contrast)\n",
    "        glcm_dissimilarities.append(dissimilarity)\n",
    "        glcm_homogeneities.append(homogeneity)\n",
    "        glcm_energies.append(energy)\n",
    "        glcm_correlations.append(correlation)\n",
    "        glcm_ASMs.append(ASM)\n",
    "    \n",
    "    df['img_size'] = img_sizes\n",
    "    df['brightness'] = img_means\n",
    "    df['contrast_std'] = img_stds\n",
    "    df['img_min'] = img_mins\n",
    "    df['img_max'] = img_maxs\n",
    "    df['contrast_range'] = df['img_max'] - df['img_min']\n",
    "    df['coefficient_of_variation'] = df['contrast_std'] / (df['brightness'] + 1e-9)\n",
    "    df['entropy'] = img_entropies\n",
    "    \n",
    "    # Add GLCM-based texture features\n",
    "    df['glcm_contrast'] = glcm_contrasts\n",
    "    df['glcm_dissimilarity'] = glcm_dissimilarities\n",
    "    df['glcm_homogeneity'] = glcm_homogeneities\n",
    "    df['glcm_energy'] = glcm_energies\n",
    "    df['glcm_correlation'] = glcm_correlations\n",
    "    df['glcm_asm'] = glcm_ASMs\n",
    "    \n",
    "    # Drop the raw 'photo' after feature extraction\n",
    "    df.drop(columns=['photo'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def extract_text_features(df):\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['text_length'] = df['text'].apply(len)\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    df['question_marks'] = df['text'].apply(lambda x: x.count('?'))\n",
    "    df['exclamation_marks'] = df['text'].apply(lambda x: x.count('!'))\n",
    "    df['periods'] = df['text'].apply(lambda x: x.count('.'))\n",
    "    df['commas'] = df['text'].apply(lambda x: x.count(','))\n",
    "    \n",
    "    df['digits_count'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df['uppercase_letters'] = df['text'].apply(lambda x: sum(c.isupper() for c in x))\n",
    "    df['uppercase_words'] = df['text'].apply(lambda x: len(re.findall(r'\\b[A-Z]+\\b', x)))\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: sum(1 for c in x if c in string.punctuation))\n",
    "    df['hashtag_count'] = df['text'].apply(lambda x: x.count('#'))\n",
    "    df['mention_count'] = df['text'].apply(lambda x: x.count('@'))\n",
    "    \n",
    "    df['unique_words_ratio'] = df.apply(\n",
    "        lambda row: len(set(row['text'].split())) / row['word_count'] if row['word_count'] > 0 else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    df['longest_word_length'] = df['text'].apply(\n",
    "        lambda x: max(len(word) for word in x.split()) if len(x.split()) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    tfidf = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "    with open('gpt1/tfidf.pickle', 'wb') as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=50)\n",
    "    text_features = svd.fit_transform(tfidf_matrix)\n",
    "    with open('gpt1/svd.pickle', 'wb') as f:\n",
    "        pickle.dump(svd, f)\n",
    "    \n",
    "    text_feature_df = pd.DataFrame(text_features, columns=[f'text_svd_{i}' for i in range(50)])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, text_feature_df], axis=1)\n",
    "    df.drop(columns=['text'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df = extract_image_features(df)\n",
    "    df = extract_text_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_vectorizer(photo_base64):\n",
    "    img = np.array(Image.open(BytesIO(base64.b64decode(photo_base64))))\n",
    "    s = img.shape\n",
    "    if len(s) == 2:\n",
    "        # Если изображение в градациях серого, повторяем по каналам\n",
    "        img = np.repeat(img[..., np.newaxis], 3, axis=2)\n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "    # Приводим изображение к виду (h*w, 3)\n",
    "    img = img.reshape(-1, 3)\n",
    "    stats = []\n",
    "    stats.append(np.array([h, w]))\n",
    "    stats.append(img.min(axis=0))\n",
    "    stats.append(img.max(axis=0))\n",
    "    stats.append(img.mean(axis=0))\n",
    "    stats.append(img.std(axis=0))\n",
    "    stats.append(np.median(img, axis=0))\n",
    "    # Корреляционная матрица – верхний треугольник (без диагонали)\n",
    "    cm = np.corrcoef(img.T)\n",
    "    stats.append(cm[np.triu_indices(len(cm), k=1)])\n",
    "    return np.concatenate(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [07:17<00:00, 45.73it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "train = data.copy()\n",
    "data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"] = train[\"text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:28<00:00, 225.12it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"img_features\"] = train[\"photo\"].progress_apply(lambda x: img_vectorizer(x))\n",
    "X_img = np.vstack(train[\"img_features\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(df):\n",
    "    img_sizes = []\n",
    "    img_means = []\n",
    "    img_stds = []\n",
    "    img_mins = []\n",
    "    img_maxs = []\n",
    "    img_entropies = []\n",
    "    \n",
    "    for img_b64 in tqdm(df['photo'].fillna(\"\")):\n",
    "        img = decode_image(img_b64)\n",
    "        \n",
    "        img_sizes.append(img.shape[0] * img.shape[1])\n",
    "        img_means.append(img.mean())\n",
    "        img_stds.append(img.std())\n",
    "        img_mins.append(img.min())\n",
    "        img_maxs.append(img.max())\n",
    "        \n",
    "        hist, _ = np.histogram(img.ravel(), bins=256, range=(0, 256))\n",
    "        hist = hist[hist > 0]\n",
    "        probabilities = hist / hist.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        img_entropies.append(entropy)\n",
    "\n",
    "    df['img_size'] = img_sizes\n",
    "    df['brightness'] = img_means \n",
    "    df['contrast_std'] = img_stds \n",
    "    df['img_min'] = img_mins\n",
    "    df['img_max'] = img_maxs\n",
    "    \n",
    "    df['contrast_range'] = df['img_max'] - df['img_min']  \n",
    "    df['coefficient_of_variation'] = df['contrast_std'] / (df['brightness'] + 1e-9)  \n",
    "    df['entropy'] = img_entropies\n",
    "    \n",
    "    df.drop(columns=['photo'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:32<00:00, 619.94it/s]\n"
     ]
    }
   ],
   "source": [
    "train = extract_image_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_img_vector = np.mean(X_img, axis=0).reshape(1, -1)\n",
    "cos_sim = cosine_similarity(X_img, avg_img_vector).flatten()\n",
    "train[\"cosine_sim\"] = cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric = pd.DataFrame(X_img)\n",
    "X_numeric[\"cosine_sim\"] = train[\"cosine_sim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = X_numeric.copy()\n",
    "X_train_all[\"text\"] = train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['like', 'comment', 'hide', 'expand', 'open_photo', 'open', 'share_to_message']\n",
    "features = [col for col in data.columns if col not in targets]\n",
    "views = data.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([data[features], X_train_all], axis=1).rename(columns=dict(zip(list(range(20)), list(map(str, list(range(20)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid, view_train, view_valid = train_test_split(all_data, data[targets], views, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [01:16<07:38, 76.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like 0.1876314150825914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [01:16<02:38, 31.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment -0.0017814723261069254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [01:17<01:09, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hide -0.00126447323057155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [02:16<01:41, 33.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand 0.5267200248799238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [02:16<00:43, 21.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_photo 0.2605927673461057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [03:08<00:32, 32.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open 0.44224149082279773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [04:18<00:00, 36.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share_to_message 0.12555175513183536\n",
      "0.2199559296723679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "score_mean = 0\n",
    "params = {\n",
    "    'like': {'iterations': 1200, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'comment': {'iterations': 700, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'hide': {'iterations': 700, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'expand': {'iterations': 900, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'open_photo': {'iterations': 500, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'open': {'iterations': 800, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'share_to_message': {'iterations': 1000, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "}\n",
    "\n",
    "for col in tqdm(targets):\n",
    "    if col == 'comment' or col == 'hide':\n",
    "        model = KNeighborsRegressor(200)\n",
    "    elif col == 'open_photo':\n",
    "        model = KNeighborsRegressor(80)\n",
    "    else:\n",
    "        model = CatBoostRegressor(**params[col], random_seed=42, text_features=['text'])\n",
    "    \n",
    "    if col == 'open_photo':\n",
    "        model.fit(X_train.drop(columns=['text']), y_train[col])\n",
    "        score = r2_score(y_valid[col], model.predict(X_valid.drop(columns=['text'])))\n",
    "    else:\n",
    "        if col in ['comment', 'hide']:\n",
    "            model.fit(X_train.drop(columns=['text']), y_train[col] / view_train)\n",
    "            score = r2_score(y_valid[col] / view_valid, model.predict(X_valid.drop(columns=['text'])))\n",
    "        else:\n",
    "            model.fit(X_train, y_train[col] / view_train)\n",
    "            score = r2_score(y_valid[col] / view_valid, model.predict(X_valid))\n",
    "    score_mean += score\n",
    "    print(col, score)\n",
    "    models[col] = model\n",
    "\n",
    "print(score_mean / len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models.pickle', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [06:59, 83.84s/it]00:00<?, ?it/s]\n",
      " 14%|█▍        | 1/7 [06:59<41:55, 419.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like: mean R2 = 0.18450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:02,  2.46it/s]\n",
      " 29%|██▊       | 2/7 [07:01<14:29, 173.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment: mean R2 = -0.00065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:02,  2.37it/s]\n",
      " 43%|████▎     | 3/7 [07:03<06:21, 95.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hide: mean R2 = 0.00345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:07, 61.55s/it]\n",
      " 57%|█████▋    | 4/7 [12:11<08:57, 179.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand: mean R2 = 0.53345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:01,  4.34it/s]\n",
      " 71%|███████▏  | 5/7 [12:12<03:50, 115.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_photo: mean R2 = 0.29057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [04:26, 53.38s/it]\n",
      " 86%|████████▌ | 6/7 [16:39<02:46, 166.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open: mean R2 = 0.45585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:35, 67.19s/it]\n",
      "100%|██████████| 7/7 [22:15<00:00, 190.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share_to_message: mean R2 = 0.11278\n",
      "Overall mean R2: 0.22571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "models = {}\n",
    "params = {\n",
    "    'like': {'iterations': 1200, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'comment': {'iterations': 700, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'hide': {'iterations': 700, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'expand': {'iterations': 900, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'open_photo': {'iterations': 500, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'open': {'iterations': 800, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'share_to_message': {'iterations': 1000, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "score_mean = 0\n",
    "fold_scores = []\n",
    "\n",
    "for col in tqdm(targets):\n",
    "    fold_r2_scores = []\n",
    "    for train_idx, valid_idx in tqdm(kf.split(all_data)):\n",
    "        X_tr, X_val = all_data.iloc[train_idx], all_data.iloc[valid_idx]\n",
    "        y_tr, y_val = data[targets][col].iloc[train_idx], data[targets][col].iloc[valid_idx]\n",
    "        view_tr, view_val = views.iloc[train_idx], views.iloc[valid_idx]\n",
    "\n",
    "        if col == 'comment' or col == 'hide':\n",
    "            model = KNeighborsRegressor(200)\n",
    "        elif col == 'open_photo':\n",
    "            model = KNeighborsRegressor(80)\n",
    "        else:\n",
    "            model = CatBoostRegressor(**params[col], random_seed=42, text_features=['text'])\n",
    "\n",
    "        if col == 'open_photo':\n",
    "            model.fit(X_tr.drop(columns=['text']), y_tr)\n",
    "            score = r2_score(y_val, model.predict(X_val.drop(columns=['text'])))\n",
    "            fold_r2_scores.append(score)\n",
    "        else:\n",
    "            if col in ['comment', 'hide']:\n",
    "                model.fit(X_tr.drop(columns=['text']), y_tr / view_tr)\n",
    "                score = r2_score(y_val / view_val, model.predict(X_val.drop(columns=['text'])))\n",
    "                fold_r2_scores.append(score)\n",
    "            else:\n",
    "                model.fit(X_tr, y_tr / view_tr)\n",
    "                score = r2_score(y_val / view_val, model.predict(X_val))\n",
    "                fold_r2_scores.append(score)\n",
    "\n",
    "    mean_r2 = np.mean(fold_r2_scores)\n",
    "    fold_scores.append(mean_r2)\n",
    "    models[col] = model\n",
    "    print(f\"{col}: mean R2 = {mean_r2:.5f}\")\n",
    "\n",
    "score_mean = np.mean(fold_scores)\n",
    "print(f\"Overall mean R2: {score_mean:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "params = {\n",
    "    'like': {'iterations': 1200, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'comment': {'iterations': 700, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'hide': {'iterations': 700, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'expand': {'iterations': 900, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'open_photo': {'iterations': 500, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'open': {'iterations': 800, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "    'share_to_message': {'iterations': 1000, 'verbose': 0, 'eval_metric': 'R2'},\n",
    "}\n",
    "\n",
    "for col in tqdm(targets):\n",
    "    if col == 'comment' or col == 'hide':\n",
    "        model = KNeighborsRegressor(200)\n",
    "    elif col == 'open_photo':\n",
    "        model = KNeighborsRegressor(80)\n",
    "    else:\n",
    "        model = CatBoostRegressor(**params[col], random_seed=42, text_features=['text'], max_depth=9)\n",
    "    \n",
    "    if col == 'open_photo':\n",
    "        model.fit(all_data.drop(columns=['text']), data[col])\n",
    "    else:\n",
    "        if col in ['comment', 'hide']:\n",
    "            model.fit(all_data.drop(columns=['text']), data[col] / views)\n",
    "        else:\n",
    "            model.fit(all_data, data[col] / views)\n",
    "    models[col] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gpt1/models.pickle', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
